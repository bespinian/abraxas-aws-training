# 9. DevOps in and to AWS

## Content

- Githooks and pipelines into AWS
- Localstack
- Quick Intro to Terraform
- The following services will be explained and focused on:
    - AppConfig
    - CDK & CloudFormation
    - CloudWatch
    - CodeArtifact
    - CodeBuild
    - CodeCommit
    - CodeDeploy
    - CodePipeline
    - FIS
    - Grafana
    - Prometheus
    - Secrets Manager
    - Service Catalog
    - Systems Manager
    - X-Ray

## Workload Account

To already gather some real world experience in how to use your own landing zone, we will treat the following hands-on as if they would be in a live productive LZ. For all the following tasks (if not specified different) work in the **sandbox/test** account.

## Amazon CloudWatch

**Amazon CloudWatch** is AWS‚Äôs observability service that provides **logging**, **metrics**, **dashboards**, and **alarms** for monitoring applications, infrastructure, and services.

It‚Äôs the default tool for tracking the health and performance of nearly everything in AWS.

### üìò Terminology

| Term            | Meaning                                                                          |
|------------------|----------------------------------------------------------------------------------|
| **Log Group**     | Container for logs from one or more sources (e.g., Lambda, ECS)                 |
| **Log Stream**    | Time-ordered sequence of log events (typically 1 per instance/container)        |
| **Metric**        | A time-series value like CPU %, error count, or latency                        |
| **Dimension**     | Metadata that identifies a metric (e.g., `FunctionName`, `InstanceId`)          |
| **Alarm**         | A condition on a metric that triggers actions (e.g., email, autoscaling)        |
| **Dashboard**     | A custom panel to visualize logs/metrics/alarms                                 |
| **Insight**       | Log query language for aggregations and filtering                               |

### Key Features

- üìä **Metrics monitoring** from all AWS services and custom apps
- üìù **Centralized logging** from Lambda, ECS, EC2, API Gateway, etc.
- üö® **Alarms & notifications** via SNS, Slack, PagerDuty, etc.
- üìà **Dashboards** to visualize logs, metrics, and alarms
- üîç **Log Insights** ‚Äî query logs with a SQL-like syntax
- üí° Native integration with X-Ray, Lambda, ECS, S3, and more

### üîç Logging vs Monitoring

| Logging                          | Monitoring                          |
|----------------------------------|--------------------------------------|
| Raw, unstructured or semi-structured data | Structured, numeric time-series data |
| Use case: debug & trace          | Use case: alert & visualize          |
| Examples: error logs, API input  | Examples: error rate, invocations/s  |

> ‚úÖ Logs show **what happened**, metrics show **how often**.

### ü™µ Logs & Log Groups

- Logs are automatically sent from:
  - Lambda, ECS, EC2 (via agent), API Gateway, Load Balancer, etc.
- Log Group: `/aws/lambda/my-function-name`
- Log retention is **configurable** (default: never expires ‚Äî be careful with costs!)
- Logs can be **searched, filtered, exported to S3**, or fed into Insights

### üß† Insights (Log Insights)

Use Log Insights for real-time investigation and log analytics:

Example query:
```sql
fields @timestamp, @message
| filter @message like /ERROR/
| sort @timestamp desc
| limit 20
```
Supports:
- Aggregation
- Time bucketing
- Field extraction (from JSON logs)

### üìè Metrics & Dimensions
- Metrics are emitted **automatically** or via `PutMetricData` API
- Every metric has:
    - A **namespace** (e.g., `AWS/Lambda`)
    - One or more **dimensions** (e.g., `FunctionName`)
    - A **unit** (Count, Seconds, Bytes, etc.)

Examples:
- `Invocations`
- `Duration`
- `Errors`
- `Throttles`

> ‚úÖ You can **publish your own metrics** from apps via the SDK

### üìä Dashboard
Create custom dashboards to visualize:
- Metrics from Lambda, EC2, API Gateway
- Alarms and statuses
- Custom metrics and logs

Supports:
- Multiple widgets per dashboard
- Period auto-refresh
- Cross-service views

### üß™ Filters
- Log filters let you define patterns to extract or match log lines
- Used in metric filters (to turn logs into metrics) and alarms
- Example pattern:
```text
?ERROR ?Exception
```
- Example filter in JSON:
```json
{ $.status = 500 }
```

### üö® Alarms
- Create alarms on:
    - Any CloudWatch metric (including custom)
    - Log-based metric filters
- Trigger:
    - SNS Topics
    - Auto Scaling
    - Lambda
    - EC2 actions
- You can set:
    - Static or anomaly-based thresholds
    - Evaluation periods
    - Notification targets

‚úÖ Now when the Lambda is invoked and fails, the alarm will fire and send you an email

### Best Practices
‚úÖ Set log **retention periods** ‚Äî don‚Äôt leave them at ‚Äúforever‚Äù   
‚úÖ Use **structured JSON logs** for better Insights queries   
‚úÖ Convert high-value logs into metrics using **metric filters**   
‚úÖ Use **dashboards** for teams/ops visibility   
‚úÖ Set up **alarms** for critical thresholds, not every fluctuation   
‚ùå Don‚Äôt log sensitive data (PII) ‚Äî CloudWatch logs are not encrypted by default beyond KMS-at-rest   

## Grafana & Prometheus

**Grafana** and **Prometheus** are popular open-source tools for **metrics visualization** and **monitoring** ‚Äî and they integrate deeply with AWS through **Amazon Managed Grafana** and **Amazon Managed Prometheus**.

They are especially powerful for teams that already use open-source observability stacks, run Kubernetes, or need **custom dashboards across multiple clouds or clusters**.

### Key Features

#### üìä Amazon Managed Grafana

- Managed version of the **Grafana dashboard platform**
- Pre-integrated with:
  - CloudWatch
  - Prometheus (self-managed or Amazon Managed)
  - X-Ray, Redshift, IoT, Athena, and more
- Supports **advanced visualizations** and alerts
- Multi-source dashboards across AWS + third-party tools (e.g., Datadog, Elasticsearch, InfluxDB)

#### üìà Amazon Managed Prometheus

- Fully managed **Prometheus-compatible metrics service**
- Scalable, high-availability backend for Prometheus metrics
- Pulls metrics from:
  - EKS (via Prometheus exporters)
  - EC2 instances
  - Custom applications
- Stores **time-series metrics** and exposes them for Grafana dashboards

> üß† Prometheus stores + queries the metrics; Grafana visualizes them.

### üîÑ Ecosystem: Grafana + Prometheus + CloudWatch

Together, they form a **flexible observability stack**:

| Component           | Role                                              |
|----------------------|---------------------------------------------------|
| **Prometheus**        | Collects + stores custom metrics (e.g., app latency, container stats) |
| **CloudWatch**        | AWS-native metrics + logs                        |
| **Grafana**           | Unified dashboards, alerting, and analysis       |

#### üîå Integration Flow Example

- Your app exposes metrics at `/metrics` (via a Prometheus exporter like `node_exporter`)
- Amazon Managed Prometheus scrapes this endpoint and stores the time-series
- Grafana connects to both:
  - **Prometheus** ‚Üí app metrics
  - **CloudWatch** ‚Üí AWS metrics
- You build dashboards that correlate EC2 CPU usage (from CloudWatch) with app latency (from Prometheus)

### Best Practices

‚úÖ Use **CloudWatch** for AWS-native metrics (Lambda, EC2, ALB, etc.)  
‚úÖ Use **Prometheus** when you:
  - Need fine-grained application or container metrics
  - Use Kubernetes/EKS  
‚úÖ Use **Grafana** as a unified view across AWS and open-source data  
‚úÖ Manage access via **IAM Identity Center (SSO)** or **SAML**  
‚úÖ Set **retention periods and alerting rules** carefully ‚Äî Prometheus scales fast  
‚ùå Don‚Äôt store logs in Prometheus ‚Äî it‚Äôs for metrics only

## Infrastructure as Code (IaC)

IaC lets you manage your AWS resources using code ‚Äî bringing version control, automation, repeatability, and reduced human error to your infrastructure setup.

AWS provides **CloudFormation** and **CDK** natively. Many teams also prefer **Terraform** for its flexibility, ecosystem, and multi-cloud capabilities.

### üß± CloudFormation & CDK

#### Key Features

**CloudFormation (CFN)**:
- Native IaC tool from AWS
- YAML or JSON-based
- Declarative (you describe *what* you want, not *how*)
- Handles ordering, dependencies, and rollback automatically
- Integrated with IAM, CloudWatch, and every AWS service
- Supports **Change Sets** and **StackSets**

**AWS CDK (Cloud Development Kit)**:
- Abstraction layer on top of CloudFormation
- Write IaC in Python, TypeScript, Java, C#, or Go
- Synthesizes to CloudFormation templates behind the scenes
- Reusable, testable, parameterized infrastructure "constructs"
- Allows for many functionalities that CFN misses (like the possiblity to loop resource creation instead of defining same resource over and over again -> Code Duplication)

> ‚úÖ CDK = CloudFormation, but with code and logic  
> ‚ùå But you‚Äôre still bound to CloudFormation's limits (slow deploys, YAML output)

#### Hands-On: Deploying S3 with CloudFormation (Vanilla)

> Create a simple S3 bucket using raw YAML and deploy via the AWS Console or CLI

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: Create a basic S3 bucket

Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: my-training-bucket-demo
```
Deploy via AWS Console or with:
```bash
aws cloudformation deploy \ --template-file s3.yaml \ --stack-name simple-s3-demo
```
‚úÖ Result: A working S3 bucket defined in code   

### üåç Terraform
**Key Features**
- Developed by HashiCorp (open-source + commercial)
- Declarative and **provider-based** (supports AWS, Azure, GCP, etc.)
- Written in **HCL** (HashiCorp Configuration Language)
- Large ecosystem of **modules, plugins, and community support**
- State file keeps track of real-world vs declared resources
- Flexible remote backends (e.g., S3 + DynamoDB for state locking)
- Supports **plan / apply** lifecycle

> ‚úÖ Terraform works well in **multi-cloud, modular, DevOps-heavy environments**

### ü•ä Terraform vs AWS Native Stack (CFN/CDK)

| Feature / Concern        | CloudFormation          | CDK                      | Terraform                   |
| ------------------------ | ----------------------- | ------------------------ | --------------------------- |
| Language                 | YAML/JSON               | TypeScript, Python, etc. | HCL                         |
| Learning curve           | Medium                  | Easy (for devs)          | Easy (for DevOps)           |
| Ecosystem                | AWS-only                | AWS-only                 | Multi-cloud                 |
| Speed of feedback        | ‚ùå Slow deploys          | ‚ùå Slow (still CFN)       | ‚úÖ Faster plan/apply         |
| Reusability / Modularity | ‚ùå Limited               | ‚úÖ Constructs             | ‚úÖ Modules + registry        |
| Tooling / Editor support | Limited                 | ‚úÖ Strong (modern langs)  | ‚úÖ Excellent                 |
| Community & Resources    | AWS docs                | Growing community        | ‚úÖ Massive ecosystem         |
| Best for‚Ä¶                | Cloud-native-only infra | Dev teams close to AWS   | ‚úÖ Multi-cloud, mature CI/CD |
| Lock-in                  | High                    | High                     | ‚úÖ Lower                     |

### üí¨ Honest Take
- **CloudFormation is stable**, predictable, but **painfully verbose** and slow. Best if you‚Äôre in full AWS-native environments and want 100% support with no third-party tools. But in general it's made for machine understanding and code generation, <ins>**not for humans to write it**!</ins>
- **CDK is great for developers** and made for humans, but harder for ops engineers and DevOps pipelines. You write code, but you still suffer from **slow CloudFormation updates** underneath. Also, its abstractions can **get in your way** as projects scale.
- Terraform is usually preferred by:
    - DevOps teams
    - CI/CD-heavy workflows
    - Teams using multiple clouds or external services

> üî• CDK is shiny, but Terraform is the <ins>**swiss standard**</ins>, **battle-tested** and **scales better** across teams, clouds, and environments.

### Best Practices
‚úÖ Use **version control** for all infrastructure   
‚úÖ Use **modules** (TF) or **constructs** (CDK) to reuse components   
‚úÖ Enable **change reviews** using Terraform Plan or CloudFormation Change Sets   
‚úÖ Store state securely (e.g., S3 + DynamoDB for TF)   
‚úÖ Avoid ‚Äúdrift‚Äù ‚Äî check actual state regularly   
‚úÖ Terraform is the **swiss standard**, so use Terraform in most cases!

## CodeCommit

**AWS CodeCommit** is a fully managed **Git repository service** hosted in AWS. It provides private, secure source control that integrates tightly with other AWS services (IAM, CodeBuild, CodePipeline, etc.).

### Key Features

- ‚úÖ Fully managed **Git-compatible** repositories
- üîê Integrated with **IAM for fine-grained access control**
- üíæ Stores any file type, supports large repositories
- üîÑ Supports **branching, merging, pull requests**
- üì° Accessible via HTTPS or SSH
- üîí Encrypted at rest and in transit (KMS + TLS)
- üìà Supports **triggers** (Lambda, SNS, SQS) on push/merge

### ü§î CodeCommit vs GitHub / GitLab

| Feature                      | CodeCommit                    | GitHub / GitLab              |
|-----------------------------|-------------------------------|-------------------------------|
| Hosting                     | AWS-native                    | External (GitHub, SaaS/Self-hosted) |
| IAM integration             | ‚úÖ Yes (native)               | ‚ùå No (requires PATs or SAML) |
| UI/UX & community           | ‚ùå Minimal                    | ‚úÖ Excellent                  |
| Webhooks & CI/CD hooks      | SNS, Lambda, CodePipeline     | Webhooks, GitHub Actions, CI |
| Ideal for                   | Regulated, AWS-secure workloads | Open source, external collaboration |
| Multi-region replication    | ‚ùå Not built-in               | ‚ùå Not common either          |

> ‚úÖ Use CodeCommit when:
> - You need **tight security with IAM**
> - You want everything in AWS (no third-party vendors)
> ‚ùå Avoid it for:
> - Public projects
> - Developer experience (GitHub UI is far superior)

### Hands-On: Repository Prep for Later Pipeline

> Goal: Create a CodeCommit repository with a sample app ready for CI/CD use in CodeBuild or CodePipeline.

1. **If you know Terraform, then do the following tasks with the proper Terraform resources instead of the management console!** 
2. Create the Repo
3. Clone and Initialize
```bash
git clone https://git-codecommit.<region>.amazonaws.com/v1/repos/my-training-repo
cd my-training-repo
```
4. Add Sample Files
```bash
echo "Hello from AWS CodeCommit!" > index.html
echo 'version: 0.2\nphases:\n  build:\n    commands:\n      - echo "Building..."' > buildspec.yml
```
5. Commit and push
```bash
git add .
git commit -m "Initial commit for training"
git push origin main
```
‚úÖ Now your CodeCommit repo is ready for use with CodeBuild, CodePipeline, or manual deployments.

### Best Practices
‚úÖ Use **branch protection** and **pull request workflows**
‚úÖ Integrate with **CloudWatch Events** or **SNS** for repo triggers
‚úÖ Use **encrypted connections (HTTPS/SSH)** with IAM auth
‚úÖ Set **repository policies** to restrict push/merge actions
‚ùå Don‚Äôt use for public repos ‚Äî there‚Äôs no public repo hosting

## AWS CodeBuild

**AWS CodeBuild** is a fully managed **build service** that compiles source code, runs tests, and produces deployable artifacts ‚Äî without managing any build servers.

It works inside CodePipeline or standalone, and supports a wide range of languages and environments via prebuilt or custom Docker images.

### üõ°Ô∏è Key Features

- üß™ **Fully serverless** ‚Äî no need to provision CI runners or agents
- üì¶ Supports builds for:
  - Java, Python, Node.js, Go, .NET, Ruby, Docker, and more
- üßæ Uses a `buildspec.yml` to define steps (similar to GitHub Actions or GitLab CI)
- üîÅ Integrated with:
  - CodeCommit, GitHub, Bitbucket
  - S3 (for artifacts), ECR, CodePipeline
- üîê IAM-based permissions per project
- üß± Can run custom builds in your own **Docker containers**
- üîÑ Supports **parallel and batch builds**
- üìà Emits logs and metrics to **CloudWatch**
- üíµ Pay per minute of actual build time

### üßæ Example buildspec.yml

```yaml
version: 0.2

phases:
  install:
    commands:
      - echo Installing dependencies...
  build:
    commands:
      - echo Building the app...
      - npm run build
artifacts:
  files:
    - '**/*'
```
### üí° Common Use Cases
| Use Case                      | Why Use CodeBuild?                    |
| ----------------------------- | ------------------------------------- |
| Compile and test code         | Fully managed CI, no Jenkins needed   |
| Build Docker images           | Push to ECR from CodeBuild            |
| Run linting, unit tests       | Simple to integrate with CodePipeline |
| Generate artifacts for deploy | Archive and pass to S3 or CodeDeploy  |

### Best Practices
‚úÖ Use **small, fast containers** to save build time   
‚úÖ Set **timeout limits** to avoid long-running builds   
‚úÖ Store and version your **buildspec.yml** alongside code   
‚úÖ Push **logs and metrics to CloudWatch** for diagnostics   
‚úÖ Use **environment variables** (via SSM or Secrets Manager) for secrets   
‚ùå Don‚Äôt hardcode credentials or tokens ‚Äî use IAM roles

## AWS CodeDeploy

**AWS CodeDeploy** is a fully managed **deployment automation service** that helps you deploy application changes to:

- EC2 instances
- On-premise servers
- Lambda functions
- ECS services (blue/green deployments)

It's designed to reduce downtime and make updates predictable, auditable, and automated ‚Äî but it often comes with **extra setup complexity**.

### Key Features

- üîÅ **Supports in-place and blue/green deployments**
  - EC2 & on-prem: can stop/start services, replace files, run hooks
  - Lambda: traffic shifting between versions/aliases
- üîÑ Rollback on failure
- üìú **AppSpec file** defines lifecycle hooks (BeforeInstall, AfterInstall, etc.)
- üß© Integrates with:
  - EC2 Auto Scaling
  - CodePipeline
  - S3 (for artifacts)
  - GitHub / CodeCommit (for triggers)
- üìä Detailed deployment status via CloudWatch & Events
- üîê IAM-controlled deployments and instance roles

### üìú Example appspec.yml (for EC2)

```yaml
version: 0.0
os: linux
files:
  - source: /
    destination: /var/www/html
hooks:
  AfterInstall:
    - location: scripts/configure.sh
      timeout: 180
      runas: ec2-user
```

### üí° When to Use CodeDeploy
| Scenario                        | CodeDeploy Fit?    | Why                                       |
| ------------------------------- | ------------------ | ----------------------------------------- |
| Deploying to EC2 Auto Scaling   | ‚úÖ Yes              | Hook into instance lifecycle              |
| Gradual rollout to Lambda       | ‚úÖ Yes              | Built-in version traffic shifting         |
| On-prem deployments             | ‚úÖ Yes              | Works with hybrid cloud or edge use cases |
| Deploying container apps on ECS | ‚úÖ (for blue/green) | Only necessary for traffic shifting       |
| Simple S3 ‚Üí Lambda deploys      | ‚ùå Overkill         | Use CodePipeline + Lambda                 |

### ‚ö†Ô∏è Things to Watch Out For
- AppSpec files must be accurate ‚Äî small errors break deployments
- In-place deployments can cause **downtime if not coordinated**
- For containers, **ECS handles most of this natively now**
- Not needed if you're using **serverless-only pipelines** (e.g., Lambda + S3 + CloudFormation)

###  Best Practices
‚úÖ Use **blue/green** for high-availability workloads   
‚úÖ Monitor all deployments via **CloudWatch and Events**   
‚úÖ Store `appspec.yml` with your app code for version control   
‚úÖ Use **lifecycle hooks** to validate, backup, or preconfigure during deployment   
‚úÖ Combine with **CodePipeline** for fully automated CI/CD   
‚ùå Don‚Äôt use in-place updates without validation or backups   

## AWS CodeArtifact

**AWS CodeArtifact** is a fully managed **artifact repository** for software packages. It lets you **store, share, and version** packages across teams ‚Äî similar to npm, PyPI, Maven Central, or NuGet ‚Äî but private and within your AWS account.

It's ideal for teams that want to host **internal libraries**, cache dependencies, or enforce vetted package usage.

### Key Features

- üì¶ Supports multiple package formats:
  - npm (JavaScript/Node.js)
  - PyPI (Python)
  - Maven (Java)
  - NuGet (.NET)
- üîÅ Acts as a **proxy/cache** for public registries
- üîê Integrated with:
  - **IAM authentication**
  - **STS tokens**
  - **CodeBuild** and **CodePipeline**
- üìÅ Scoped by **domain ‚Üí repository ‚Üí package**
- üîÑ Can automatically **fetch missing packages** from upstream (e.g., npmjs.org)
- üß™ Versioning, dependency resolution, and retention policies supported

### üß∞ Example Use Cases

| Use Case                                  | Why CodeArtifact Helps                      |
|-------------------------------------------|----------------------------------------------|
| Internal shared libraries                 | Controlled versioning + security             |
| Cache public packages for CI/CD builds    | Faster and more reliable builds              |
| Control package versions                  | Prevent risky upgrades from upstream         |
| Use private packages with CodeBuild       | No need for secrets ‚Äî IAM-based access       |

### üì• How to Authenticate

1. Use AWS CLI to get a token:
```bash
aws codeartifact login \
  --tool npm \
  --repository my-repo \
  --domain my-domain
```
2. Then use `npm install`, `pip install`, etc. ‚Äî CodeArtifact acts like a registry.
> ‚úÖ Works with CodeBuild out of the box ‚Äî no need for custom credential management.

###  Best Practices
‚úÖ Use **IAM roles + STS tokens** for secure auth (no long-lived API keys)   
‚úÖ Set up **upstream repositories** to cache public packages safely   
‚úÖ Apply **package retention policies** to limit storage cost   
‚úÖ Use **domains** to group by team or environment   
‚ùå Don‚Äôt hardcode repository URLs or tokens ‚Äî use `codeartifact login` scripts

## AWS Systems Manager

**AWS Systems Manager (SSM)** is a suite of tools that helps you **manage, patch, automate, and operate** your AWS infrastructure ‚Äî especially EC2 instances, hybrid environments, and configurations.

It acts as a **central control plane** for system operations across AWS, without needing direct SSH access.

### Key Features

#### üñ•Ô∏è Session Manager
- Secure shell access to EC2 **without SSH keys or open ports**
- Logs every session to **CloudTrail** or **S3**
- IAM-controlled access
- Supports **port forwarding** and tunneling

#### üõ†Ô∏è Run Command
- Remotely execute shell or PowerShell commands on instances
- No need to log in ‚Äî just run commands via console or CLI

#### üß© State Manager
- Enforce **desired state** (e.g., install packages, manage users)
- Useful for compliance, baseline config, patching

#### üì¶ Patch Manager
- Automates OS patching across fleets (Linux/Windows)
- Define patch baselines and maintenance windows

#### üß∞ Parameter Store
- Key-value config storage (secure + plaintext)
- Often used for app configs, feature flags, env vars
- Supports encryption with KMS

#### üìã Inventory
- Tracks installed software, patch state, instance details
- Useful for CMDB-like reporting

#### üìÅ Automation
- Create automation documents (SSM Documents) to run multi-step workflows:
  - Snapshot + patch + restart
  - Backup + deploy
  - Auto-remediation

### üõ†Ô∏è Example Use Cases

| Use Case                          | Tools Used                       |
|----------------------------------|-----------------------------------|
| Access EC2 without SSH            | Session Manager                   |
| Run update on 50 servers          | Run Command                       |
| Store app configs (e.g., `/env/`) | Parameter Store                   |
| Enforce antivirus installation    | State Manager                     |
| Auto-patch on Sundays             | Patch Manager + Maintenance Window|

### Best Practices

‚úÖ Use **Session Manager** to eliminate SSH entirely  
‚úÖ Use **Parameter Store** for app configs (or Secrets Manager for sensitive data)  
‚úÖ Enforce config via **State Manager** for compliance  
‚úÖ Schedule **Patch Manager** with **Maintenance Windows**  
‚úÖ Tag resources to group and manage them logically  
‚ùå Don‚Äôt mix up Parameter Store and Secrets Manager ‚Äî use each for what it‚Äôs best at

## AWS Secrets Manager

**AWS Secrets Manager** is a fully managed service that helps you **store, rotate, and retrieve secrets** like:

- Database credentials  
- API keys  
- OAuth tokens  
- Third-party service credentials

It allows secure, auditable access from apps and AWS services ‚Äî without storing secrets in plain text in your code or config files.

### Key Features

- üîê **Secure, encrypted secret storage**
  - All secrets are encrypted with **KMS**
- üîÅ **Automatic rotation** of secrets (optional)
  - Supports built-in integrations for RDS, Aurora, Redshift
- üîë **Fine-grained IAM access control**
  - Control *who* can retrieve *which* secret
- üì¶ **Versioning and staging labels**
  - Track current, previous, and pending versions
- üìà **Audit logs** via CloudTrail
- üß© Integrates with:
  - Lambda, EC2, ECS, RDS, CodeBuild, Terraform, etc.
  - SDKs for all major languages

### üí¨ Example Use Case: DB Credentials

1. Store credentials in Secrets Manager
2. Grant your Lambda/EC2/CodeBuild access to the secret via IAM
3. App retrieves credentials securely using AWS SDK

```python
import boto3

client = boto3.client('secretsmanager')
secret = client.get_secret_value(SecretId='prod/db-credentials')
```
> ‚úÖ Secrets are **never hardcoded**, and can be **rotated without changing code**

### üîÑ Secret Rotation
- Supports automatic rotation using Lambda
- Works out-of-the-box with:
    - RDS (MySQL, PostgreSQL, MariaDB)
    - Redshift
- You can write a **custom rotation Lambda** for other systems
- Uses staging labels: `AWSCURRENT`, `AWSPENDING`, etc.

### üîê vs. Parameter Store

| Feature             | Secrets Manager          | SSM Parameter Store     |
| ------------------- | ------------------------ | ----------------------- |
| Secret rotation     | ‚úÖ Yes                    | ‚ùå No                    |
| Audit logging       | ‚úÖ Yes                    | ‚úÖ Yes                   |
| Native secret types | ‚úÖ Yes (JSON blobs)       | ‚úÖ Yes                   |
| Cost                | üí∞ Paid (\~\$0.40/month) | ‚úÖ Free tier available   |
| Use case            | Secrets & credentials    | Config params, env vars |


### Best Practices
‚úÖ Store secrets **outside of code**   
‚úÖ Use **least-privilege IAM** for accessing secrets   
‚úÖ Enable **automatic rotation** where supported   
‚úÖ Use **KMS key policies** to restrict who can decrypt secrets   
‚úÖ Set up **alerts via CloudTrail** or EventBridge for access anomalies   
‚ùå Don‚Äôt log secrets or pass them unencrypted between services   

## AWS CodePipeline

**AWS CodePipeline** is a fully managed **CI/CD orchestration service**. It automates build, test, and deployment steps whenever code changes, so you can release software faster and more reliably.

It connects various tools and services into a **visualized pipeline** of sequential or parallel stages.

### üõ°Ô∏è Key Features

- üîÅ Automates **source ‚Üí build ‚Üí deploy** flows
- üîß Native integrations with:
  - CodeCommit, GitHub, S3 (source)
  - CodeBuild (build)
  - CodeDeploy, Lambda, ECS, CloudFormation (deploy)
- üí• Triggers on code push or webhook
- üß© Custom stages via Lambda or manual approval
- üïì Real-time pipeline execution + monitoring
- üîê IAM role per stage for least privilege

### üß± Pipeline Design in AWS

A real AWS pipeline uses a **combination of services**, depending on your architecture:

#### üß¨ Typical Flow

```text
[ Source (GitHub/CodeCommit) ]
        ‚Üì
[ Build (CodeBuild) ]
        ‚Üì
[ Deploy (Lambda, ECS, CF, CDK, Terraform) ]
        ‚Üì
[ Post-deploy (Tests, Notifications, Dashboards) ]
```

#### üì¶ Common Services in Pipeline
| Tool                     | Role in Pipeline                                | When to Use / Avoid                                      |
| ------------------------ | ----------------------------------------------- | -------------------------------------------------------- |
| **CodeBuild**            | Compile, test, bundle artifacts                 | ‚úÖ Always needed if build or test is involved             |
| **CodeDeploy**           | EC2, Lambda, or ECS deployments (with rollback) | ‚úÖ Use for EC2/ECS blue/green, ‚ùå Skip for basic S3/Lambda |
| **Secrets Manager**      | Securely inject secrets in CodeBuild or Lambda  | ‚úÖ Needed if using tokens, passwords, keys                |
| **ECR**                  | Stores Docker images for ECS/Fargate            | ‚úÖ For container-based apps                               |
| **CloudFormation / CDK** | Infrastructure deployment via IaC               | ‚úÖ Use for IaC projects, but **slow** in pipeline         |
| **Terraform**            | IaC via CodeBuild custom stage                  | ‚úÖ Great if you manage infra outside of CFN               |
> üí° You can run Terraform inside a CodeBuild stage, as it‚Äôs not natively supported like CFN/CDK.

### Hands-On: My First Complete Pipeline & Monitoring
> Goal: Push Python Lambda with Powertools ‚Üí Build & Deploy ‚Üí Track ‚ÄúStar Wars‚Äù mentions ‚Üí Visualize on a CloudWatch dashboard

1. **If you know Terraform, then do the following tasks with the proper Terraform resources instead of the management console!** 
2. Create a repo with and push a function code to it that
    - Counts with powertools "Star Wars" in the event text
    - Publishes a custom metric
3. Create the buildspec that installs the python packages properly as lambda layer
4. Create the Pipeline with the CodeCommit as source, the buildstage on the buildspec and on the deploy stage a CloudFormation that creates the Lambda function
5. Push the lambda code into the repository and it should be deployed now
6. Now run the lambda multiple times
7.  Create a CloudWatch Dashboard:
    - Add a widget for `Custom/Lambda/StarWarsMentions` or however your metric was named
    - Set it to display latest invocation metric in real time

### ‚úÖ Result
Now, every push to your repo:
- Builds the Lambda
- Deploys it automatically
- Runs and emits metrics
- Updates a CloudWatch dashboard

### Best Practices
‚úÖ Separate **dev, test, prod** pipelines with environment variables   
‚úÖ Use **IAM roles per stage** with scoped permissions   
‚úÖ Store build artifacts in S3 or deploy to Lambda Layers   
‚úÖ Integrate with **manual approval stages** for prod   
‚úÖ Use **CloudWatch Logs + Events** for pipeline alerts   
‚ùå Don‚Äôt hardcode config ‚Äî use Parameter Store or Secrets Manager   

## Git Hooks from Outside

Many teams already host their code on **GitHub** or **GitLab**, but still want to **trigger deployments or CI/CD pipelines inside AWS**. The most common way to do this is through **webhooks** ‚Äî HTTP POST calls sent to AWS when something happens (e.g., code pushed, pull request merged).

### üêô GitHub Webhooks

GitHub supports **repository webhooks** to notify AWS services:

- Events supported:
  - `push`, `pull_request`, `release`, etc.
- Can send to:
  - API Gateway ‚Üí Lambda
  - CodePipeline (via custom webhook URL)
  - EventBridge (via GitHub ‚Üí API Gateway bridge)

#### Native Integration with CodePipeline

If you're using CodePipeline:
- ‚úÖ GitHub (and GitHub Enterprise Cloud) is supported **natively**
- OAuth token required for connection
- Triggers on **commit to specific branch**

> Best for small, straightforward CI/CD pipelines hosted in GitHub

### ü¶ä GitLab Webhooks

GitLab (SaaS or self-hosted) also supports outbound webhooks:

- Triggered on:
  - Pushes, tags, merges, pipeline events
- Can call:
  - Lambda (via API Gateway)
  - Step Functions
  - SQS for decoupling
- No native AWS integration ‚Äî must set up manually via API Gateway or EventBridge proxy

> GitLab is powerful, but less natively integrated ‚Äî expect more wiring.

### Hands-On: GitHub Push Triggers AWS Pipeline

> Goal: Push to a GitHub repo ‚Üí trigger an AWS CodePipeline

1. Create a Pipeline (no source yet)
2. Use the AWS Console, Terraform or CLI to create a basic CodePipeline with:
    - Build + Deploy stages
    - **No source configured yet**
3. Create a Custom Webhook
```bash
aws codepipeline create-webhook \
  --name github-webhook \
  --target-pipeline my-training-pipeline \
  --target-action mySource \
  --filters '[{"jsonPath":"$.ref", "matchEquals":"refs/heads/main"}]' \
  --authentication GITHUB_HMAC \
  --authentication-configuration SecretToken=<your-token>
```
4. Save the returned webhook URL.
5. Add a Webhook to GitHub
Go to your GitHub repo:
- Settings ‚Üí Webhooks ‚Üí Add webhook
- Payload URL: `https://...` (from AWS CLI output)
- Content type: `application/json`
- Secret: same as `SecretToken`
- Events: select `push`
- Save

‚úÖ Result: Every push to main triggers your AWS pipeline.   

### Best Practices
‚úÖ Use **HMAC secrets** to verify webhook authenticity   
‚úÖ Route webhooks through **API Gateway + Lambda** if validation/custom logic needed   
‚úÖ Use **SQS or EventBridge** Pipes to decouple triggers from execution   
‚úÖ For GitHub, prefer native integration unless you need full control   
‚ùå Don‚Äôt expose webhooks publicly without verification ‚Äî they can be abused

## AWS AppConfig

**AWS AppConfig** is a managed service for **application configuration management** and **feature flag rollouts**. It‚Äôs part of AWS Systems Manager and helps you deploy configuration changes **safely and gradually**, without redeploying your code.

Think of it like a control plane for runtime application behavior.

### Key Features

- üß© Deploy **runtime configuration data** independently of code
- ‚öôÔ∏è Designed for **feature flags**, tuning parameters, and dynamic settings
- üîê Validates config before rollout (JSON schema, Lambda validators)
- üß™ Supports **canary deployments**, **linear rollouts**, or full rollout
- üìä Built-in monitoring with CloudWatch alarms
- üßµ Integrated with:
  - Lambda
  - ECS/Fargate
  - EC2
  - Mobile/web apps (via SDK or API)
- üí• Abort and rollback if metrics indicate problems

### üß∞ Example Use Cases

- Toggle features on/off in production
- Gradually roll out a new UI layout
- Change application behavior (limits, thresholds) without redeploying
- Emergency disablement ("kill switches")

### üóÇÔ∏è How It Works

1. **Create an App** (logical grouping)
2. **Create an Environment** (dev, prod, etc.)
3. **Define a Configuration Profile**
   - Can source config from:
     - SSM Parameter Store
     - Secrets Manager
     - S3
     - Inline JSON
4. **Deploy the Configuration**
   - With validation (optional)
   - With controlled rollout strategy
   - With monitoring & rollback options

### Best Practices

‚úÖ Use **schemas and validators** to avoid broken rollouts  
‚úÖ Store **runtime configs**, not secrets (use Secrets Manager for credentials)  
‚úÖ Monitor rollout health with **CloudWatch alarms**  
‚úÖ Use **feature flags** to separate deployment from release  
‚ùå Don‚Äôt use AppConfig as a general-purpose config store ‚Äî it‚Äôs for **controlled runtime flags**   

## AWS Fault Injection Simulator (FIS)

**AWS Fault Injection Simulator (FIS)** is a managed **chaos engineering service** that lets you safely test how your workloads respond to faults, like instance terminations, latency injection, API throttling, or network isolation.

It helps improve **resilience, fault tolerance, and observability** ‚Äî especially in production-like environments.

### Key Features

- üí• Simulates real-world failures:
  - Stop/terminate EC2 instances
  - Kill ECS tasks
  - Introduce CPU/network stress
  - Inject latency or packet loss in VPC
  - Throttle SSM, DynamoDB, or API Gateway
- ‚öôÔ∏è Controlled **scoped experiments** with rollback support
- üß™ Define actions + targets using:
  - Resource tags
  - Auto Scaling groups
  - SSM integrations
- üß© Integrates with:
  - CloudWatch (for alarms)
  - EventBridge (for alerts)
  - Systems Manager (to run commands pre/post fault)
- üß† Common use cases:
  - Test if auto-scaling kicks in
  - Validate retry logic
  - Observe monitoring system reactions

### üß¨ How FIS Integrates with CI/CD Pipelines

While FIS is often used in **pre-prod or staging**, you can integrate it into **post-deployment phases** of a CI/CD pipeline to test if resilience mechanisms are in place.

#### Example CI/CD Flow:

```text
[ Build & Deploy ] ‚Üí [ Smoke Tests ] ‚Üí [ Chaos Experiment (FIS) ] ‚Üí [ Verify Observability ]
```
- ‚úÖ After deploying an app via **CodePipeline**, run an **FIS experiment** to:
    - Kill one EC2 instance
    - Drop packets between two subnets
- ‚úÖ Then validate that:
    - Auto Scaling replaced the instance
    - The service stayed available
    - Alarms fired and recovered

#### Integration Points:
- Trigger FIS experiment from a CodeBuild step using CLI:
```bash
aws fis start-experiment --experiment-template-id <template-id>
```
- Use **CloudWatch alarms** to auto-abort the experiment if impact exceeds tolerance
- Add a **manual approval** stage after the chaos test if needed

## AWS Service Catalog

**AWS Service Catalog** lets administrators create and manage **approved collections of resources** that teams can deploy in a self-service, controlled way.

It‚Äôs especially useful in **enterprise environments** where teams need to use vetted infrastructure (with security, cost, and compliance baked in) ‚Äî but still want autonomy in launching services.

### Key Features

- üß± Define **products** (CloudFormation stacks) with optional parameters
- üóÇ Group products into **portfolios**
- üîê Use **IAM and SSO permissions** to control who can launch what
- üß© Launch templates include:
  - EC2 with hardening
  - Pre-approved S3/Lambda setups
  - Full VPCs, network layouts, databases
- üîÅ Version control of products (rollback possible)
- ‚úÖ Users can deploy from console or CLI ‚Äî no YAML editing needed

> üß† Think: ‚ÄúInternal AWS Marketplace‚Äù for infrastructure blueprints

### üóÇÔ∏è App Registry

The **AppRegistry** within Service Catalog helps **group and track resources** by application ‚Äî useful for **inventory, compliance, and governance**.

- Link CloudFormation stacks to logical **applications**
- Assign **owners, environments, business metadata**
- Useful for:
  - Tag-based automation
  - App-level reporting
  - Cross-team transparency

> üîç AppRegistry = **meta-layer for governance and org-wide reporting**

### Example Use Cases

| Use Case                                      | Why Service Catalog Helps                  |
|-----------------------------------------------|---------------------------------------------|
| Developers need secure base templates         | Admins publish hardened EC2/S3 templates    |
| Maintain versioned VPC/Lambda blueprints      | Controlled stack updates                    |
| Allow self-service, but avoid free-for-all    | IAM limits + pre-configured options         |
| Central ops team wants app-level visibility   | Use App Registry to map stacks to owners    |

### Best Practices

‚úÖ Use **tagging and AppRegistry** for lifecycle and audit clarity  
‚úÖ Keep **products up to date** (especially for networking/security baselines)  
‚úÖ Define **clear naming and parameter rules**  
‚úÖ Grant **least privilege access** to portfolios via IAM  
‚ùå Don‚Äôt use Service Catalog if your teams are small, flexible, or constantly iterating ‚Äî it slows down rapid prototyping

## AWS X-Ray

**AWS X-Ray** is a **distributed tracing service** that helps you understand how your application is behaving end-to-end ‚Äî across microservices, Lambda, APIs, and databases.

It collects traces from your application, shows latency bottlenecks, and lets you **pinpoint where requests fail, slow down, or branch**.

### Key Features

- üìç Trace **individual requests** across services and infrastructure
- üß† Understand:
  - Latency breakdown (e.g., 100ms in Lambda, 200ms in RDS)
  - Service dependencies
  - Outliers and anomalies
- üîÑ Works with:
  - Lambda
  - API Gateway
  - EC2, ECS, and Fargate (with agents)
  - SDK-integrated apps (e.g., Python, Node.js)
- üìà Visual **service maps**, trace timelines, and error rate views
- üîå Integrated with CloudWatch and CloudTrail

### üîß How to Integrate X-Ray into CI/CD

While **X-Ray isn‚Äôt part of the pipeline execution itself**, you enable tracing **during or after deployment** ‚Äî so you can observe the behavior of new releases in real time.

#### In a typical pipeline:

```text
[ CodeCommit ] ‚Üí [ CodeBuild ] ‚Üí [ CodeDeploy / Lambda ] ‚Üí [ X-Ray enabled application ]
```

**How to enable:**
- ‚úÖ For **Lambda:** just toggle `Active tracing` in deployment config or CLI:

```bash
aws lambda update-function-configuration \
  --function-name myLambda \
  --tracing-config Mode=Active
```

- ‚úÖ For **ECS/EC2 apps:**
    - Install the **X-Ray daemon or sidecar**
    - Export trace headers in requests
    - Use SDK to instrument app code
- ‚úÖ During deploy via CodePipeline/CDK:
    - Add X-Ray flags in **CloudFormation/CDK constructs**
    - Or use `aws xray` CLI in a post-deploy CodeBuild step

> üí° Use this to **auto-enable tracing** as part of your delivery process, and alert on spikes or failures post-deploy.

### ‚ö†Ô∏è Considerations ‚Äî When to Use and When It‚Äôs Overkill
| Scenario                               | X-Ray Fit? | Why                                          |
| -------------------------------------- | ---------- | -------------------------------------------- |
| Microservice debugging                 | ‚úÖ Yes      | Traces request paths across services         |
| Lambda-based async workflows           | ‚úÖ Yes      | Shows cold start latency and downstream deps |
| Small monolith or low-traffic app      | ‚ùå No       | Logs + metrics are sufficient                |
| Real-time debugging with low tolerance | ‚úÖ Yes      | Combine with alarms to catch regression fast |
| Just need uptime/error checks          | ‚ùå No       | CloudWatch alone will do                     |

### Best Practices
‚úÖ Enable **Active tracing** for key functions/services only (don‚Äôt overdo it)   
‚úÖ Use **sampling** rules to limit trace volume and cost   
‚úÖ Add **metadata and annotations** for custom filtering   
‚úÖ Monitor trace anomalies post-deploy ‚Äî great for **canary analysis**   
‚úÖ Combine with **CloudWatch Logs Insights** for full observability   
‚ùå Don‚Äôt treat X-Ray as a logging tool ‚Äî it‚Äôs for performance tracing   

## Bespinians most used Ranking

‚úÖ **CloudWWatch** for logs and basic monitoring   
‚úÖ Full **CodePipeline** based on customers DevOps strategy   
‚úÖ Full **CodBuild** even outside of the basic Pipeliine to build dependencies like automated **Lambda Layer** creation  
‚úÖ **Systems Manager** for multiple of its functionalities    
‚úÖ **Secrets Manager** to store secrets for CICD Pipelines and automated workflows   
‚úÖ **X-Ray** if we really need tracing (rarely but then effectively)